{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21202b81",
   "metadata": {},
   "source": [
    "Понял! Если test.csv используется для предсказания на Kaggle, то данные из этого файла не содержат метки (label), и его нужно обрабатывать иначе. Вместо этого train.csv будет использоваться для обучения модели, а test.csv — для выполнения предсказаний.\n",
    "\n",
    "Давайте пересмотрим подход с учетом этого. В этом случае:\n",
    "\t1.\ttrain.csv — используется для обучения модели и содержит метки (label).\n",
    "\t2.\ttest.csv — используется только для предсказания, меток (label) в нем нет.\n",
    "\n",
    "Пошаговое объяснение:\n",
    "\t•\tМы обучаем модель на данных из train.csv.\n",
    "\t•\tМы выполняем предсказания для данных в test.csv.\n",
    "\t•\tРезультаты предсказаний сохраняются в CSV файл, включая ID (или другие уникальные идентификаторы), текст и предсказанные метки.\n",
    "\n",
    "1. Пример структуры файлов:\n",
    "\n",
    "train.csv:\n",
    "\n",
    "text,category,numeric,label\n",
    "\"Текст новости о политике\", \"Politics\", 5, 0\n",
    "\"Технологические достижения\", \"Technology\", 10, 1\n",
    "\"Исследования в области медицины\", \"Health\", 8, 0\n",
    "...\n",
    "\n",
    "test.csv (без label):\n",
    "\n",
    "text,category,numeric\n",
    "\"Инновации в медицине\", \"Health\", 6\n",
    "\"Последние достижения в IT\", \"Technology\", 12\n",
    "\"Политика и экономика\", \"Politics\", 7\n",
    "...\n",
    "\n",
    "2. Код с обработкой данных и сохранением предсказаний\n",
    "\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# Загрузка данных из CSV\n",
    "train_data = pd.read_csv('train.csv')  # Путь к файлу train.csv\n",
    "test_data = pd.read_csv('test.csv')    # Путь к файлу test.csv\n",
    "\n",
    "# Разделение данных на X (все признаки) и y (метки) для обучения\n",
    "X_train = train_data.drop('label', axis=1)\n",
    "y_train = train_data['label']\n",
    "\n",
    "# Применяем данные из test.csv для предсказаний\n",
    "X_test = test_data.drop('label', axis=1, errors='ignore')  # В test.csv нет 'label'\n",
    "\n",
    "# 1. Пайплайн для обработки текстовых, категориальных и числовых данных\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', TfidfVectorizer(stop_words='english', max_features=5000), 'text'),  # Обработка текста\n",
    "        ('category', OneHotEncoder(), ['category']),  # OneHot Encoding для категориальных данных\n",
    "        ('numeric', StandardScaler(), ['numeric'])  # Нормализация для числовых данных\n",
    "    ])\n",
    "\n",
    "# Пайплайн с предварительной обработкой и моделью\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Обучение модели\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Предсказания для тестовой выборки\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Печать отчет по меткам для train данных (если у вас есть метки для теста)\n",
    "# Например, если у вас есть метки в train.csv:\n",
    "# print(classification_report(y_train, y_pred))\n",
    "\n",
    "# 2. Интерпретация модели с использованием SHAP\n",
    "# SHAP используется для интерпретации модели, обученной на пайплайне\n",
    "explainer = shap.Explainer(pipeline.named_steps['classifier'], pipeline.named_steps['preprocessor'].transform(X_train))\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Показать summary plot для SHAP\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "# 3. Интерпретация модели с использованием LIME\n",
    "# Для LIME потребуется передать комбинированные данные (текст, категориальные, числовые)\n",
    "X_test_processed = pipeline.named_steps['preprocessor'].transform(X_test)\n",
    "\n",
    "# Инициализируем LIME объяснитель\n",
    "explainer_lime = LimeTabularExplainer(\n",
    "    training_data=X_train_processed,\n",
    "    feature_names=np.concatenate([pipeline.named_steps['preprocessor'].transformers_[0][1].get_feature_names_out(),  # для текста\n",
    "                                  pipeline.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out(),  # для категорий\n",
    "                                  ['numeric']]),  # для числовых данных\n",
    "    class_names=['Class 0', 'Class 1'],\n",
    "    discretize_continuous=True\n",
    ")\n",
    "\n",
    "# Пример объяснения одного текста с помощью LIME\n",
    "i = 42  # Индекс для тестового примера\n",
    "exp = explainer_lime.explain_instance(X_test_processed[i], pipeline.predict_proba, num_features=10)\n",
    "\n",
    "# Показать результаты\n",
    "exp.show_in_notebook()\n",
    "\n",
    "# Сохранение предсказаний в CSV для Kaggle\n",
    "predictions_df = pd.DataFrame({\n",
    "    'id': test_data.index,  # Идентификатор примера (например, индекс в тестовом наборе)\n",
    "    'predicted_label': y_pred  # Предсказанные метки\n",
    "})\n",
    "\n",
    "# Сохраняем в CSV файл для Kaggle\n",
    "predictions_df.to_csv('predictions.csv', index=False)\n",
    "\n",
    "print(\"Предсказания сохранены в файл 'predictions.csv'\")\n",
    "\n",
    "Важные моменты:\n",
    "\t1.\tЗагрузка данных:\n",
    "\t•\tМы загружаем train.csv и test.csv.\n",
    "\t•\tДля train.csv данные делятся на признаки (X_train) и метки (y_train).\n",
    "\t•\tДля test.csv мы используем только признаки, так как меток в нем нет.\n",
    "\t2.\tПайплайн с ColumnTransformer:\n",
    "\t•\tИспользуется ColumnTransformer для обработки разных типов данных.\n",
    "\t•\tTF-IDF применяется к текстовым данным.\n",
    "\t•\tOneHotEncoder для категориальных признаков.\n",
    "\t•\tStandardScaler для числовых признаков.\n",
    "\t3.\tПредсказания:\n",
    "\t•\tПосле обучения модели на train.csv, мы выполняем предсказания для данных из test.csv.\n",
    "\t4.\tSHAP и LIME:\n",
    "\t•\tИспользуются для интерпретации модели.\n",
    "\t•\tSHAP показывает важность признаков, а LIME дает локальные объяснения для одного примера.\n",
    "\t5.\tСохранение предсказаний в CSV:\n",
    "\t•\tВ test.csv могут быть уникальные идентификаторы (например, индекс строк). Мы сохраняем эти идентификаторы в колонке id в CSV файле для подачи на Kaggle.\n",
    "\t•\tВ CSV файл сохраняются только столбцы: id и predicted_label.\n",
    "\n",
    "Пример содержимого predictions.csv:\n",
    "\n",
    "id,predicted_label\n",
    "0,0\n",
    "1,1\n",
    "2,0\n",
    "...\n",
    "\n",
    "Заметка:\n",
    "\t•\tВ Kaggle часто требуется, чтобы файл предсказаний содержал только id и predicted_label. Если в вашем test.csv есть колонка с уникальными идентификаторами, используйте её вместо индексов.\n",
    "\n",
    "Если у вас есть дополнительные вопросы или нужна помощь в адаптации под конкретные требования, не стесняйтесь обращаться!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db77ecb",
   "metadata": {},
   "source": [
    "Да, конечно! Включение временных рядов (Time Series) в ваш пайплайн возможно, и есть несколько способов объединить обработку текстовых данных, категориальных и числовых признаков с временными рядами.\n",
    "\n",
    "Когда вы работаете с временными рядами, данные обычно имеют временную зависимость, то есть значения одного признака могут зависеть от его предыдущих значений во времени. Это особенно важно для задач предсказания, где данные имеют явно выраженную последовательность.\n",
    "\n",
    "Стратегии для работы с временными рядами:\n",
    "\t1.\tИспользование временных признаков: Вы можете добавить такие признаки, как “период времени” (например, день, месяц, год), скользящие средние, разности между последовательными значениями и т.д.\n",
    "\t2.\tИспользование временных окон: Время можно преобразовать в лаги (например, значения за предыдущие n периодов времени), что позволяет модели учитывать прошлые значения.\n",
    "\t3.\tИспользование RNN или LSTM: Для более сложных задач можно использовать нейронные сети, такие как RNN или LSTM, которые могут обрабатывать временную зависимость.\n",
    "\n",
    "Как объединить обработку временных рядов с другими признаками (например, текст, категории и числа):\n",
    "\t1.\tДанные: Мы будем работать с текстовыми данными (например, новости), категориальными (например, категории) и числовыми (например, значения, связанные с временными метками). Важно учитывать, что для временных рядов, помимо обычных признаков, важно учитывать историческую информацию.\n",
    "\t2.\tВременные признаки: Например, для временных рядов можно добавить такие признаки, как день недели, месяц, скользящие средние, разности и т. д. Эти признаки могут быть полезны при предсказаниях.\n",
    "\t3.\tПреобразование и подача данных в модель: Временные признаки можно обрабатывать с помощью ColumnTransformer, а также добавить преобразование для временных рядов (например, лаги или разности).\n",
    "\n",
    "Пример кода с временными признаками:\n",
    "\n",
    "Предположим, у нас есть следующая структура:\n",
    "\t•\ttrain.csv: Содержит данные для обучения с временными признаками, текстовыми данными, категориальными и числовыми.\n",
    "\t•\ttest.csv: Содержит тестовые данные без меток, только для предсказания.\n",
    "\n",
    "date,text,category,numeric,label\n",
    "2022-01-01,Текст новости о политике,Politics,5,0\n",
    "2022-01-02,Технологические достижения,Technology,10,1\n",
    "2022-01-03,Исследования в области медицины,Health,8,0\n",
    "...\n",
    "\n",
    "Как объединить обработку временных рядов с другими признаками:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import shap\n",
    "\n",
    "# Загрузка данных из CSV\n",
    "train_data = pd.read_csv('train.csv')  # Путь к файлу train.csv\n",
    "test_data = pd.read_csv('test.csv')    # Путь к файлу test.csv\n",
    "\n",
    "# Преобразуем 'date' в datetime формат\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "\n",
    "# Добавление временных признаков (день недели, месяц, год, день месяца)\n",
    "train_data['day_of_week'] = train_data['date'].dt.dayofweek\n",
    "train_data['month'] = train_data['date'].dt.month\n",
    "train_data['year'] = train_data['date'].dt.year\n",
    "train_data['day_of_month'] = train_data['date'].dt.day\n",
    "\n",
    "test_data['day_of_week'] = test_data['date'].dt.dayofweek\n",
    "test_data['month'] = test_data['date'].dt.month\n",
    "test_data['year'] = test_data['date'].dt.year\n",
    "test_data['day_of_month'] = test_data['date'].dt.day\n",
    "\n",
    "# Убираем 'date' из признаков\n",
    "X_train = train_data.drop(['label', 'date'], axis=1)\n",
    "y_train = train_data['label']\n",
    "X_test = test_data.drop(['date'], axis=1)\n",
    "\n",
    "# 1. Пайплайн для обработки текстовых, категориальных и числовых данных + временные признаки\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', TfidfVectorizer(stop_words='english', max_features=5000), 'text'),  # Обработка текста\n",
    "        ('category', OneHotEncoder(), ['category']),  # OneHot Encoding для категориальных данных\n",
    "        ('numeric', StandardScaler(), ['numeric', 'day_of_week', 'month', 'year', 'day_of_month'])  # Нормализация для числовых данных и временных признаков\n",
    "    ])\n",
    "\n",
    "# Пайплайн с предварительной обработкой и моделью\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Обучение модели\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Предсказания для тестовой выборки\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Печать отчета по меткам для train данных (если у вас есть метки для теста)\n",
    "# Например, если у вас есть метки в train.csv:\n",
    "# print(classification_report(y_train, y_pred))\n",
    "\n",
    "# 2. Интерпретация модели с использованием SHAP\n",
    "explainer = shap.Explainer(pipeline.named_steps['classifier'], pipeline.named_steps['preprocessor'].transform(X_train))\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Показать summary plot для SHAP\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "# 3. Интерпретация модели с использованием LIME\n",
    "# Для LIME потребуется передать комбинированные данные (текст, категориальные, числовые, временные)\n",
    "X_test_processed = pipeline.named_steps['preprocessor'].transform(X_test)\n",
    "\n",
    "# Инициализируем LIME объяснитель\n",
    "explainer_lime = LimeTabularExplainer(\n",
    "    training_data=X_train_processed,\n",
    "    feature_names=np.concatenate([pipeline.named_steps['preprocessor'].transformers_[0][1].get_feature_names_out(),  # для текста\n",
    "                                  pipeline.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out(),  # для категорий\n",
    "                                  ['numeric', 'day_of_week', 'month', 'year', 'day_of_month']]),  # для числовых и временных данных\n",
    "    class_names=['Class 0', 'Class 1'],\n",
    "    discretize_continuous=True\n",
    ")\n",
    "\n",
    "# Пример объяснения одного текста с помощью LIME\n",
    "i = 42  # Индекс для тестового примера\n",
    "exp = explainer_lime.explain_instance(X_test_processed[i], pipeline.predict_proba, num_features=10)\n",
    "\n",
    "# Показать результаты\n",
    "exp.show_in_notebook()\n",
    "\n",
    "# Сохранение предсказаний в CSV для Kaggle\n",
    "predictions_df = pd.DataFrame({\n",
    "    'id': test_data.index,  # Идентификатор примера (например, индекс в тестовом наборе)\n",
    "    'predicted_label': y_pred  # Предсказанные метки\n",
    "})\n",
    "\n",
    "# Сохраняем в CSV файл для Kaggle\n",
    "predictions_df.to_csv('predictions.csv', index=False)\n",
    "\n",
    "print(\"Предсказания сохранены в файл 'predictions.csv'\")\n",
    "\n",
    "Объяснение:\n",
    "\t1.\tПризнаки времени:\n",
    "\t•\tМы извлекаем различные признаки из временной метки, такие как день недели, месяц, год, день месяца.\n",
    "\t•\tЭти признаки могут быть полезны для моделирования сезонных изменений или трендов в данных.\n",
    "\t2.\tОбработка признаков:\n",
    "\t•\tВременные признаки добавляются как числовые признаки и обрабатываются с помощью StandardScaler.\n",
    "\t•\tОстальные текстовые и категориальные признаки обрабатываются соответствующими трансформерами (TF-IDF для текста и OneHotEncoding для категорий).\n",
    "\t3.\tИнтерпретация модели:\n",
    "\t•\tМы используем SHAP для глобальной интерпретации, чтобы понять, какие признаки (в том числе временные) влияют на предсказания модели.\n",
    "\t•\tLIME используется для локальной интерпретации модели, чтобы объяснить предсказания для конкретных примеров.\n",
    "\t4.\tСохранение результатов:\n",
    "\t•\tПредсказания сохраняются в файл predictions.csv, который может быть отправлен на Kaggle или использован для дальнейшего анализа.\n",
    "\n",
    "Возможные улучшения:\n",
    "\t•\tВместо использования простых временных признаков, таких как день недели или месяц, вы можете использовать скользящие средние или разности между значениями (например, разность между текущим и предыдущим значением).\n",
    "\t•\tДля сложных временных зависимостей можно использовать рекуррентные нейронные сети (RNN, LSTM, GRU), которые хорошо справляются с анализом последовательностей данных.\n",
    "\n",
    "Этот подход позволит вам использовать как текстовые, категориальные, числовые данные, так и временные признаки для улучшения качества предсказаний. Если нужно более глубоко интегрировать временные ряды, например, через рекуррентные нейронные сети, дайте знать, и я помогу адаптировать решение!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f05fe19",
   "metadata": {},
   "source": [
    "Для добавления rolling window (скользящего окна) в обработку временных рядов, вы можете создать дополнительные признаки, которые будут вычислять агрегированные значения (например, среднее, стандартное отклонение, максимум и минимум) по скользящему окну на основе временных данных.\n",
    "\n",
    "Это полезно, когда вы хотите, чтобы модель учитывала не только текущее состояние, но и информацию о недавних значениях (например, скользящее среднее или другие статистики за последние n периодов).\n",
    "\n",
    "Пример:\n",
    "\n",
    "Добавим скользящее среднее и другие статистики, используя окно (например, размер окна = 3, что означает использование данных за последние 3 дня).\n",
    "\n",
    "Как это сделать:\n",
    "\t1.\tДобавим скользящее среднее (rolling mean), стандартное отклонение (rolling std), максимум и минимум для числовых и временных признаков.\n",
    "\t2.\tПрименим эти агрегированные признаки к данным, чтобы улучшить модель.\n",
    "\n",
    "Обновленный код с добавлением скользящего окна:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import shap\n",
    "\n",
    "# Загрузка данных из CSV\n",
    "train_data = pd.read_csv('train.csv')  # Путь к файлу train.csv\n",
    "test_data = pd.read_csv('test.csv')    # Путь к файлу test.csv\n",
    "\n",
    "# Преобразуем 'date' в datetime формат\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "\n",
    "# Добавление временных признаков (день недели, месяц, год, день месяца)\n",
    "train_data['day_of_week'] = train_data['date'].dt.dayofweek\n",
    "train_data['month'] = train_data['date'].dt.month\n",
    "train_data['year'] = train_data['date'].dt.year\n",
    "train_data['day_of_month'] = train_data['date'].dt.day\n",
    "\n",
    "test_data['day_of_week'] = test_data['date'].dt.dayofweek\n",
    "test_data['month'] = test_data['date'].dt.month\n",
    "test_data['year'] = test_data['date'].dt.year\n",
    "test_data['day_of_month'] = test_data['date'].dt.day\n",
    "\n",
    "# Убираем 'date' из признаков\n",
    "X_train = train_data.drop(['label', 'date'], axis=1)\n",
    "y_train = train_data['label']\n",
    "X_test = test_data.drop(['date'], axis=1)\n",
    "\n",
    "# Применяем скользящие статистики (например, окно размером 3)\n",
    "window_size = 3\n",
    "\n",
    "# Для тренировки и теста добавляем скользящее среднее и другие статистики\n",
    "def add_rolling_features(data, window=3):\n",
    "    data['rolling_mean'] = data['numeric'].rolling(window=window, min_periods=1).mean()\n",
    "    data['rolling_std'] = data['numeric'].rolling(window=window, min_periods=1).std()\n",
    "    data['rolling_max'] = data['numeric'].rolling(window=window, min_periods=1).max()\n",
    "    data['rolling_min'] = data['numeric'].rolling(window=window, min_periods=1).min()\n",
    "    return data\n",
    "\n",
    "# Добавляем скользящие признаки для тренировочных и тестовых данных\n",
    "train_data = add_rolling_features(train_data, window_size)\n",
    "test_data = add_rolling_features(test_data, window_size)\n",
    "\n",
    "# Обновляем X_train и X_test с новыми признаками\n",
    "X_train = train_data.drop(['label', 'date'], axis=1)\n",
    "X_test = test_data.drop(['date'], axis=1)\n",
    "\n",
    "# 1. Пайплайн для обработки текстовых, категориальных и числовых данных + временные признаки и скользящие статистики\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', TfidfVectorizer(stop_words='english', max_features=5000), 'text'),  # Обработка текста\n",
    "        ('category', OneHotEncoder(), ['category']),  # OneHot Encoding для категориальных данных\n",
    "        ('numeric', StandardScaler(), ['numeric', 'day_of_week', 'month', 'year', 'day_of_month', 'rolling_mean', 'rolling_std', 'rolling_max', 'rolling_min'])  # Нормализация для числовых данных и временных признаков\n",
    "    ])\n",
    "\n",
    "# Пайплайн с предварительной обработкой и моделью\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Обучение модели\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Предсказания для тестовой выборки\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Печать отчета по меткам для train данных (если у вас есть метки для теста)\n",
    "# Например, если у вас есть метки в train.csv:\n",
    "# print(classification_report(y_train, y_pred))\n",
    "\n",
    "# 2. Интерпретация модели с использованием SHAP\n",
    "explainer = shap.Explainer(pipeline.named_steps['classifier'], pipeline.named_steps['preprocessor'].transform(X_train))\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Показать summary plot для SHAP\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "# 3. Интерпретация модели с использованием LIME\n",
    "# Для LIME потребуется передать комбинированные данные (текст, категориальные, числовые, временные)\n",
    "X_test_processed = pipeline.named_steps['preprocessor'].transform(X_test)\n",
    "\n",
    "# Инициализируем LIME объяснитель\n",
    "explainer_lime = LimeTabularExplainer(\n",
    "    training_data=X_train_processed,\n",
    "    feature_names=np.concatenate([pipeline.named_steps['preprocessor'].transformers_[0][1].get_feature_names_out(),  # для текста\n",
    "                                  pipeline.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out(),  # для категорий\n",
    "                                  ['numeric', 'day_of_week', 'month', 'year', 'day_of_month', 'rolling_mean', 'rolling_std', 'rolling_max', 'rolling_min']]),  # для числовых и временных данных\n",
    "    class_names=['Class 0', 'Class 1'],\n",
    "    discretize_continuous=True\n",
    ")\n",
    "\n",
    "# Пример объяснения одного текста с помощью LIME\n",
    "i = 42  # Индекс для тестового примера\n",
    "exp = explainer_lime.explain_instance(X_test_processed[i], pipeline.predict_proba, num_features=10)\n",
    "\n",
    "# Показать результаты\n",
    "exp.show_in_notebook()\n",
    "\n",
    "# Сохранение предсказаний в CSV для Kaggle\n",
    "predictions_df = pd.DataFrame({\n",
    "    'id': test_data.index,  # Идентификатор примера (например, индекс в тестовом наборе)\n",
    "    'predicted_label': y_pred  # Предсказанные метки\n",
    "})\n",
    "\n",
    "# Сохраняем в CSV файл для Kaggle\n",
    "predictions_df.to_csv('predictions.csv', index=False)\n",
    "\n",
    "print(\"Предсказания сохранены в файл 'predictions.csv'\")\n",
    "\n",
    "Пояснение:\n",
    "\t1.\tДобавление скользящих признаков:\n",
    "\t•\tМы создаем дополнительные признаки, используя метод rolling для числовых данных:\n",
    "\t•\trolling_mean: скользящее среднее.\n",
    "\t•\trolling_std: стандартное отклонение по скользящему окну.\n",
    "\t•\trolling_max и rolling_min: максимальное и минимальное значение по окну.\n",
    "\t2.\tОбновление признаков:\n",
    "\t•\tМы обновляем train_data и test_data, добавляя новые агрегированные признаки.\n",
    "\t•\tДалее эти признаки включаются в X_train и X_test.\n",
    "\t3.\tПредобработка данных:\n",
    "\t•\tДля обработки скользящих признаков и других числовых признаков мы используем StandardScaler.\n",
    "\t•\tДля текстовых и категориальных признаков применяем TfidfVectorizer и OneHotEncoder, соответственно.\n",
    "\t4.\tSHAP и LIME:\n",
    "\t•\tИспользуем SHAP для глобальной интерпретации модели и LIME для локальной интерпретации.\n",
    "\t•\tЭти методы помогают понять, как именно скользящие статистики и другие признаки влияют на решение модели.\n",
    "\t5.\tСохранение предсказаний:\n",
    "\t•\tПредсказания сохраняются в predictions.csv для Kaggle или дальнейшего анализа.\n",
    "\n",
    "Дополнительно:\n",
    "\t•\tВы можете изменять размер окна в rolling(window=3), например, увеличить его до 7 или 14 дней, чтобы учитывать более долгосрочные зависимости.\n",
    "\t•\tТакже можно добавить другие виды агрегированных признаков, такие как skewness (асимметрия распределения), kurtosis (эксцесс), которые могут быть полезны для анализа временных рядов.\n",
    "\n",
    "Этот подход позволит вам включить временные зависимости в ваши признаки и использовать их для построения более точных моделей."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
