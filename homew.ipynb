{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17acbaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "import pickle\n",
    "import random\n",
    "import lightgbm as lgb\n",
    "# from sklearn.model_selection import train_test_split # Not strictly needed for this implementation\n",
    "from math import ceil, floor\n",
    "from datetime import timedelta # Импортируем timedelta\n",
    "import psutil # Для мониторинга памяти, если нужно\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 0. Конфигурация и Настройка Директорий ---\n",
    "print(\"--- Stage 0: Configuration & Setup ---\")\n",
    "dir_ = '/kaggle/input/kazakhstan-ai-respa-take-home' # <--- ИЗМЕНИТЕ НА ВАШ КОРНЕВОЙ КАТАЛОГ\n",
    "# Если запускаете локально, задайте полные пути или относительные от места запуска скрипта\n",
    "raw_data_dir = os.path.join(dir_) # Пример пути\n",
    "processed_data_dir = os.path.join('/kaggle/working/', 'processed')\n",
    "model_dir = os.path.join(\"/kaggle/working/\", 'models')\n",
    "submission_dir = os.path.join('/kaggle/working/', 'submissions')\n",
    "\n",
    "# Создание папок\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(submission_dir, exist_ok=True)\n",
    "print(f\"Raw data expected in: {os.path.abspath(raw_data_dir)}\")\n",
    "print(f\"Processed data will be saved to: {os.path.abspath(processed_data_dir)}\")\n",
    "print(f\"Models will be saved to: {os.path.abspath(model_dir)}\")\n",
    "print(f\"Submissions will be saved to: {os.path.abspath(submission_dir)}\")\n",
    "\n",
    "\n",
    "TARGET = 'num_papers'\n",
    "P_HORIZON = 8  # Прогнозируем на 8 недель\n",
    "SEED = 42\n",
    "HIST_END_DATE = pd.to_datetime('2025-02-09') # Дата конца исторических данных\n",
    "\n",
    "# Конфигурация признаков\n",
    "lag_weeks = [1, 2, 3, 4, 8, 12, 52] # Лаги в неделях\n",
    "rolling_windows = [4, 8, 12, 26, 52] # Окна для скользящих средних/std\n",
    "lag_for_roll = 1 # Сдвиг перед расчетом скользящих окон\n",
    "validation_weeks = P_HORIZON # Сколько недель использовать для валидации перед обучением рекурсивной модели\n",
    "recursive_history_weeks = 52 * 2 # Сколько недель истории включать в данные для рекурсивного прогноза\n",
    "\n",
    "# LGBM Параметры (стартовые, ТРЕБУЮТ НАСТРОЙКИ!)\n",
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'tweedie', # или 'poisson'/'regression_l2'\n",
    "    'tweedie_variance_power': 1.1, # Настроить на валидации (1-2)\n",
    "    'metric': 'rmse',\n",
    "    'subsample': 0.7,\n",
    "    'subsample_freq': 1,\n",
    "    'learning_rate': 0.02, # Настроить\n",
    "    'num_leaves': 2**7 - 1, # Настроить\n",
    "    'min_data_in_leaf': 2**7 - 1, # Настроить\n",
    "    'feature_fraction': 0.7,\n",
    "    'max_bin': 100, # Обычно достаточно для признаков с небольшим числом уникальных значений\n",
    "    'n_estimators': 3000, # Управляется через early stopping\n",
    "    'boost_from_average': False,\n",
    "    'verbose': -1,\n",
    "    'seed': SEED,\n",
    "    'num_threads': -1 # Использовать все доступные ядра\n",
    "}\n",
    "early_stopping_rounds = 50 # Для early stopping\n",
    "\n",
    "# --- Вспомогательные Функции ---\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    # Также полезно для torch, tensorflow, если они используются\n",
    "    # try:\n",
    "    #     import torch\n",
    "    #     torch.manual_seed(seed)\n",
    "    #     torch.cuda.manual_seed(seed)\n",
    "    #     torch.backends.cudnn.deterministic = True\n",
    "    #     torch.backends.cudnn.benchmark = False\n",
    "    # except ImportError:\n",
    "    #     pass\n",
    "    # try:\n",
    "    #     import tensorflow as tf\n",
    "    #     tf.random.set_seed(seed)\n",
    "    # except ImportError:\n",
    "    #     pass\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if pd.isna(c_min) or pd.isna(c_max): # Пропускаем, если есть NaN, т.к. они могут влиять на min/max типов\n",
    "                continue\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                       df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                # Проверяем NaN явно перед конвертацией\n",
    "                # Осторожно с float16, может привести к потере точности\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                     # Только если очень уверены в отсутствии проблем с точностью/NaN\n",
    "                     # df[col] = df[col].astype(np.float16)\n",
    "                     # Вместо этого используем float32 как более безопасный вариант\n",
    "                     df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "# Функция для рекурсивного расчета rolling features\n",
    "def make_lag_roll(df, target_col, group_col, shift_day, roll_wind):\n",
    "    col_name = f'rolling_mean_{shift_day}_{roll_wind}'\n",
    "    df[col_name] = df.groupby(group_col)[target_col].shift(shift_day).rolling(roll_wind, min_periods=1).mean().astype(np.float16) # min_periods=1, чтобы избежать NaN в начале рекурсии\n",
    "    # Добавим std тоже\n",
    "    col_name_std = f'rolling_std_{shift_day}_{roll_wind}'\n",
    "    df[col_name_std] = df.groupby(group_col)[target_col].shift(shift_day).rolling(roll_wind, min_periods=2).std().astype(np.float16) # min_periods=2 для std\n",
    "    return df[[col_name, col_name_std]]\n",
    "\n",
    "# --- Этап 1: Препроцессинг и Инжиниринг Признаков ---\n",
    "def run_preprocessing():\n",
    "    print(\"\\n--- Running Stage 1: Preprocessing & Feature Engineering ---\")\n",
    "    overall_start_time = time.time()\n",
    "\n",
    "    # 1. Загрузка Данных\n",
    "    print(\"Loading data...\")\n",
    "    try:\n",
    "        # ИЗМЕНИТЕ ИМЕНА ФАЙЛОВ НА ВАШИ!\n",
    "        history_df = pd.read_csv(os.path.join(raw_data_dir, 'train.csv')) # ИЛИ arxiv_history.csv\n",
    "        history_df.rename(columns={'week':'absolute_week_id', 'value':'num_papers'}, inplace=True, errors='ignore') # Пример переименования\n",
    "        assert 'absolute_week_id' in history_df.columns\n",
    "        assert 'category' in history_df.columns\n",
    "        assert TARGET in history_df.columns\n",
    "\n",
    "        test_df_template = pd.read_csv(os.path.join(raw_data_dir, 'test-7.csv')) # Шаблон\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: Required input file not found: {e}. Exiting.\")\n",
    "        sys.exit()\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading data: {e}. Exiting.\")\n",
    "        sys.exit()\n",
    "\n",
    "    history_df = reduce_mem_usage(history_df)\n",
    "    test_df_template = reduce_mem_usage(test_df_template)\n",
    "\n",
    "    LAST_HIST_WEEK = history_df['absolute_week_id'].max()\n",
    "    FIRST_PRED_WEEK = LAST_HIST_WEEK + 1\n",
    "    TOTAL_WEEKS_TO_GENERATE = LAST_HIST_WEEK + P_HORIZON\n",
    "    print(f\"Historical data ends at week {LAST_HIST_WEEK}. Prediction starts week {FIRST_PRED_WEEK}.\")\n",
    "\n",
    "    # 2. Генерация Календаря\n",
    "    print(\"Generating calendar...\")\n",
    "    # Используем существующие week ID, если есть в истории\n",
    "    min_hist_week = history_df['absolute_week_id'].min()\n",
    "    all_weeks = range(min_hist_week, TOTAL_WEEKS_TO_GENERATE + 1)\n",
    "    calendar_df = pd.DataFrame({'absolute_week_id': all_weeks})\n",
    "\n",
    "    # Предполагаем, что у нас НЕТ точных дат, работаем с номерами\n",
    "    # Если есть даты, используйте метод с timedelta из прошлого ответа\n",
    "    calendar_df['year'] = (calendar_df['absolute_week_id'] / 52.1775).astype(int) + 2021 # Примерное вычисление года\n",
    "    calendar_df['week_of_year'] = ((calendar_df['absolute_week_id'] - 1) % 52).astype(int) + 1 # Примерное\n",
    "    calendar_df['month'] = ((calendar_df['week_of_year'] - 1) // (52 / 12)).astype(int) + 1\n",
    "    calendar_df['quarter'] = ((calendar_df['month'] - 1) // 3).astype(int) + 1\n",
    "\n",
    "    # Циклические признаки\n",
    "    calendar_df['week_sin'] = np.sin(2 * np.pi * calendar_df['week_of_year'] / 52.1775).astype(np.float32)\n",
    "    calendar_df['week_cos'] = np.cos(2 * np.pi * calendar_df['week_of_year'] / 52.1775).astype(np.float32)\n",
    "    calendar_df['month_sin'] = np.sin(2 * np.pi * calendar_df['month'] / 12).astype(np.float32)\n",
    "    calendar_df['month_cos'] = np.cos(2 * np.pi * calendar_df['month'] / 12).astype(np.float32)\n",
    "\n",
    "    calendar_df = reduce_mem_usage(calendar_df, verbose=False)\n",
    "\n",
    "    # 3. Создание Базовой Сетки\n",
    "    print(\"Creating base grid...\")\n",
    "    categories = history_df['category'].unique()\n",
    "    future_grid = []\n",
    "    for week_pred_rel in range(1, P_HORIZON + 1):\n",
    "        abs_week = FIRST_PRED_WEEK + week_pred_rel - 1\n",
    "        for cat in categories:\n",
    "            future_grid.append([cat, week_pred_rel, abs_week, np.nan])\n",
    "\n",
    "    future_df = pd.DataFrame(future_grid, columns=['category', 'week_id', 'absolute_week_id', TARGET])\n",
    "    future_df = reduce_mem_usage(future_df, verbose=False)\n",
    "\n",
    "    grid_df = pd.concat([history_df[['category', 'absolute_week_id', TARGET]], future_df.drop(columns=['week_id'])]).reset_index(drop=True)\n",
    "    grid_df = grid_df.merge(future_df[['category', 'absolute_week_id', 'week_id']], on=['category', 'absolute_week_id'], how='left')\n",
    "    del future_df; gc.collect()\n",
    "\n",
    "    # 4. Добавление Календарных Признаков\n",
    "    print(\"Adding calendar features...\")\n",
    "    grid_df = grid_df.merge(calendar_df, on='absolute_week_id', how='left')\n",
    "    grid_df = reduce_mem_usage(grid_df, verbose=False)\n",
    "\n",
    "    # 5. Создание Лаговых Признаков\n",
    "    print(\"Creating lag features...\")\n",
    "    grid_df = grid_df.sort_values(by=['category', 'absolute_week_id'])\n",
    "    for lag in lag_weeks:\n",
    "        grid_df[f'{TARGET}_lag_{lag}'] = grid_df.groupby('category')[TARGET].shift(lag).astype(np.float16)\n",
    "    grid_df = reduce_mem_usage(grid_df, verbose=False)\n",
    "\n",
    "    # 6. Создание Скользящих Признаков\n",
    "    print(\"Creating rolling features...\")\n",
    "    for window in rolling_windows:\n",
    "        print(f\" Rolling window: {window}\")\n",
    "        grid_df[f'rolling_mean_{lag_for_roll}_{window}'] = grid_df.groupby('category')[TARGET].shift(lag_for_roll).rolling(window, min_periods=max(1, ceil(window/4))).mean().astype(np.float16)\n",
    "        grid_df[f'rolling_std_{lag_for_roll}_{window}'] = grid_df.groupby('category')[TARGET].shift(lag_for_roll).rolling(window, min_periods=max(2, ceil(window/4))).std().astype(np.float16)\n",
    "    grid_df = reduce_mem_usage(grid_df)\n",
    "\n",
    "    # 7. Создание Целевого Кодирования\n",
    "    print(\"Creating mean encoding features...\")\n",
    "    # Кодирование будет рассчитано перед обучением каждой модели на правильном срезе данных\n",
    "\n",
    "    # 8. Финальная обработка и сохранение\n",
    "    print(\"Finalizing features and saving...\")\n",
    "    grid_df = grid_df.sort_values(by=['category', 'absolute_week_id']).reset_index(drop=True)\n",
    "\n",
    "    # Определяем признаки для модели (кроме идентификаторов и цели)\n",
    "    # `category` оставим для LabelEncoding внутри обучения\n",
    "    cols_to_drop = ['absolute_week_id', 'week_id', TARGET] # Список колонок НЕ используемых как признаки\n",
    "    model_features = [col for col in grid_df.columns if col not in cols_to_drop and col != 'category']\n",
    "    categorical_features = ['year', 'week_of_year', 'month', 'quarter'] # Добавить другие, если есть\n",
    "    # Добавляем саму 'category' к категориальным для передачи в LGBM\n",
    "    categorical_for_lgbm = categorical_features + ['category_encoded']\n",
    "\n",
    "    print(\"Feature List:\", model_features)\n",
    "    print(\"Categorical Features for Model:\", categorical_for_lgbm)\n",
    "\n",
    "    # Сохранение итоговой сетки\n",
    "    grid_path = os.path.join(processed_data_dir, 'grid_arxiv_features.pkl')\n",
    "    grid_df.to_pickle(grid_path)\n",
    "    print(f\"Feature grid saved to {grid_path}. Shape: {grid_df.shape}\")\n",
    "    print(f\"Memory usage: {grid_df.memory_usage().sum() / 1024**2:.2f} Mb\")\n",
    "    print(f\"--- Stage 1: Completed in {(time.time() - overall_start_time)/60:.2f} min ---\")\n",
    "    gc.collect()\n",
    "    return grid_df, model_features, categorical_for_lgbm, categories\n",
    "\n",
    "# --- Этап 2: Обучение Моделей ---\n",
    "def run_training(grid_df, model_features, categorical_for_lgbm, categories):\n",
    "    print(\"\\n--- Running Stage 2: Model Training ---\")\n",
    "    overall_start_time = time.time()\n",
    "    seed_everything(SEED)\n",
    "\n",
    "    # Используем validation_weeks для определения валидационного периода\n",
    "    VALID_START_WEEK = FIRST_PRED_WEEK - validation_weeks\n",
    "    print(f\"Training up to week {VALID_START_WEEK-1}\")\n",
    "    print(f\"Validating on weeks {VALID_START_WEEK} to {FIRST_PRED_WEEK-1}\")\n",
    "\n",
    "    # Сохраняем данные для рекурсивного предсказания (история + будущие NaN)\n",
    "    print(\"Preparing data for recursive prediction...\")\n",
    "    start_hist_recursive = FIRST_PRED_WEEK - recursive_history_weeks\n",
    "    weeks_for_recursive_test = range(start_hist_recursive, FIRST_PRED_WEEK + P_HORIZON)\n",
    "    recursive_base_data = grid_df[grid_df['absolute_week_id'].isin(weeks_for_recursive_test)].copy()\n",
    "    recursive_base_data.loc[recursive_base_data['absolute_week_id'] >= FIRST_PRED_WEEK, TARGET] = np.nan\n",
    "    recursive_test_path = os.path.join(processed_data_dir, 'test_recursive_arxiv.pkl')\n",
    "    recursive_base_data.to_pickle(recursive_test_path)\n",
    "    print(f\"Data for recursive prediction saved. Shape: {recursive_base_data.shape}\")\n",
    "    del recursive_base_data; gc.collect()\n",
    "\n",
    "    # Цикл обучения по категориям\n",
    "    trained_models_recursive = {}\n",
    "    trained_models_nonrecursive = {}\n",
    "\n",
    "    # Применим Label Encoding для 'category' один раз\n",
    "    grid_df['category_encoded'] = grid_df['category'].astype('category').cat.codes.astype('int16')\n",
    "    features_to_use = model_features + ['category_encoded']\n",
    "    categorical_to_use_lgbm = [f for f in categorical_for_lgbm if f in features_to_use]\n",
    "\n",
    "    for category_code, category_name in enumerate(categories):\n",
    "        print(f\"-- Training for Category: {category_name} (Code: {category_code}) --\")\n",
    "        category_df = grid_df[grid_df['category_encoded'] == category_code]\n",
    "\n",
    "        # Перерасчет mean-encoding для избежания утечек на валидации\n",
    "        # Обычно делается в K-Fold CV, здесь упрощенный вариант\n",
    "        # Рассчитываем на данных строго ДО НАЧАЛА валидационного сета\n",
    "        encoding_train_mask = category_df['absolute_week_id'] < VALID_START_WEEK\n",
    "        if encoding_train_mask.sum() > 0:\n",
    "             mean_enc = category_df.loc[encoding_train_mask, TARGET].mean()\n",
    "             std_enc = category_df.loc[encoding_train_mask, TARGET].std()\n",
    "             category_df['enc_category_mean'] = mean_enc.astype(np.float16)\n",
    "             category_df['enc_category_std'] = std_enc.fillna(0).astype(np.float16) # fillna если только 1 значение в истории\n",
    "        else:\n",
    "             category_df['enc_category_mean'] = 0.0\n",
    "             category_df['enc_category_std'] = 0.0\n",
    "\n",
    "        # Удаление строк с NaN в лагах/роллингах ТОЛЬКО ДЛЯ ОБУЧАЮЩЕЙ части\n",
    "        # Не затрагивает валидацию и будущее\n",
    "        lag_check_col = f'{TARGET}_lag_{max(lag_weeks)}' # Самый длинный лаг\n",
    "        roll_check_col = f'rolling_mean_{lag_for_roll}_{max(rolling_windows)}' # Пример роллинга\n",
    "        check_cols = [col for col in [lag_check_col, roll_check_col] if col in category_df.columns]\n",
    "\n",
    "        train_mask_nr = category_df['absolute_week_id'] < VALID_START_WEEK\n",
    "        valid_mask_nr = (category_df['absolute_week_id'] >= VALID_START_WEEK) & (category_df['absolute_week_id'] < FIRST_PRED_WEEK)\n",
    "\n",
    "        # Очищаем NaN из трейна для обеих моделей (делаем это тут, чтобы не копировать дважды)\n",
    "        train_df_clean = category_df[train_mask_nr].dropna(subset=check_cols).copy()\n",
    "        valid_df = category_df[valid_mask_nr].copy() # На валидации NaN не убираем\n",
    "\n",
    "        # 1. Обучение нерекурсивной модели\n",
    "        if not train_df_clean.empty and not valid_df.empty:\n",
    "            print(f\"   Training non-recursive model... Train shape: {train_df_clean.shape}, Valid shape: {valid_df.shape}\")\n",
    "            train_data_nr = lgb.Dataset(train_df_clean[features_to_use],\n",
    "                                        label=train_df_clean[TARGET],\n",
    "                                        categorical_feature=[f for f in categorical_to_use_lgbm if f in features_to_use])\n",
    "            valid_data_nr = lgb.Dataset(valid_df[features_to_use],\n",
    "                                        label=valid_df[TARGET],\n",
    "                                        categorical_feature=[f for f in categorical_to_use_lgbm if f in features_to_use],\n",
    "                                        reference=train_data_nr)\n",
    "\n",
    "            estimator_nr = lgb.train(lgb_params,\n",
    "                                     train_data_nr,\n",
    "                                     valid_sets=[valid_data_nr],\n",
    "                                     callbacks=[lgb.early_stopping(stopping_rounds=early_stopping_rounds, verbose=100)], # Вывод при улучшении\n",
    "                                     num_boost_round=lgb_params['n_estimators'])\n",
    "\n",
    "            model_path_nr = os.path.join(model_dir, f'lgb_model_nonrecursive_{category_name}.bin')\n",
    "            estimator_nr.save_model(model_path_nr)\n",
    "            trained_models_nonrecursive[category_name] = estimator_nr # Сохраняем модель\n",
    "            print(f\"   Non-recursive model saved to {model_path_nr}. Best Iteration: {estimator_nr.best_iteration}\")\n",
    "            del train_data_nr, valid_data_nr, estimator_nr; gc.collect()\n",
    "        else:\n",
    "             print(\"   Skipping non-recursive: Empty train or valid set after NaN removal.\")\n",
    "\n",
    "        # 2. Обучение рекурсивной модели (на всех данных до FIRST_PRED_WEEK)\n",
    "        train_mask_r = category_df['absolute_week_id'] < FIRST_PRED_WEEK\n",
    "        train_df_r_clean = category_df[train_mask_r].dropna(subset=check_cols).copy()\n",
    "        # Используем тот же валидационный сет для early stopping\n",
    "        valid_df_r = valid_df # Мы его уже скопировали\n",
    "\n",
    "        if not train_df_r_clean.empty and not valid_df_r.empty:\n",
    "            print(f\"   Training recursive model... Train shape: {train_df_r_clean.shape}, Valid shape: {valid_df_r.shape}\")\n",
    "            train_data_r = lgb.Dataset(train_df_r_clean[features_to_use],\n",
    "                                       label=train_df_r_clean[TARGET],\n",
    "                                       categorical_feature=[f for f in categorical_to_use_lgbm if f in features_to_use])\n",
    "            valid_data_r_lgb = lgb.Dataset(valid_df_r[features_to_use],\n",
    "                                           label=valid_df_r[TARGET],\n",
    "                                           categorical_feature=[f for f in categorical_to_use_lgbm if f in features_to_use],\n",
    "                                           reference=train_data_r)\n",
    "\n",
    "            estimator_r = lgb.train(lgb_params,\n",
    "                                    train_data_r,\n",
    "                                    valid_sets=[valid_data_r_lgb],\n",
    "                                    callbacks=[lgb.early_stopping(stopping_rounds=early_stopping_rounds, verbose=100)],\n",
    "                                    num_boost_round=lgb_params['n_estimators'])\n",
    "\n",
    "            model_path_r = os.path.join(model_dir, f'lgb_model_recursive_{category_name}.bin')\n",
    "            estimator_r.save_model(model_path_r)\n",
    "            trained_models_recursive[category_name] = estimator_r\n",
    "            print(f\"   Recursive model saved to {model_path_r}. Best Iteration: {estimator_r.best_iteration}\")\n",
    "            del train_data_r, valid_data_r_lgb, estimator_r, train_df_r_clean, valid_df_r; gc.collect()\n",
    "        else:\n",
    "            print(\"   Skipping recursive: Empty train or valid set after NaN removal.\")\n",
    "\n",
    "        del category_df; gc.collect()\n",
    "\n",
    "    print(f\"\\n--- Stage 2: Completed in {(time.time() - overall_start_time)/60:.2f} min ---\")\n",
    "    return trained_models_recursive, trained_models_nonrecursive, features_to_use, categorical_to_use_lgbm\n",
    "\n",
    "\n",
    "# --- Этап 3: Генерация Прогнозов ---\n",
    "def run_prediction(trained_models_recursive, trained_models_nonrecursive, features_to_use, categories):\n",
    "    print(\"\\n--- Starting Stage 3: Prediction Generation ---\")\n",
    "    overall_start_time = time.time()\n",
    "\n",
    "    # Загрузка данных (полная сетка или только нужные для теста + истории)\n",
    "    grid_df = pd.read_pickle(os.path.join(processed_data_dir, 'grid_arxiv_features.pkl'))\n",
    "    grid_df['category_encoded'] = grid_df['category'].astype('category').cat.codes.astype('int16')\n",
    "    grid_df = reduce_mem_usage(grid_df, verbose=False)\n",
    "\n",
    "    # 1. Нерекурсивный Прогноз\n",
    "    print(\" Generating non-recursive predictions...\")\n",
    "    nonrecursive_preds_list = []\n",
    "    test_mask = grid_df['absolute_week_id'] >= FIRST_PRED_WEEK\n",
    "\n",
    "    for category_name in categories:\n",
    "        print(f\"   Predicting non-recursive for: {category_name}\")\n",
    "        if category_name in trained_models_nonrecursive:\n",
    "            model_nr = trained_models_nonrecursive[category_name]\n",
    "            category_code = grid_df.loc[grid_df['category']==category_name, 'category_encoded'].iloc[0] # Получаем код\n",
    "            category_test_features = grid_df[(grid_df['category_encoded'] == category_code) & test_mask]\n",
    "\n",
    "            if not category_test_features.empty:\n",
    "                predictions = model_nr.predict(category_test_features[features_to_use], num_iteration=model_nr.best_iteration)\n",
    "                # Копируем предсказания в новый DF, чтобы избежать SettingWithCopyWarning\n",
    "                preds_df = category_test_features[['category', 'absolute_week_id', 'week_id']].copy()\n",
    "                preds_df[TARGET] = predictions.clip(0)\n",
    "                nonrecursive_preds_list.append(preds_df)\n",
    "            else:\n",
    "                print(f\"   Warning: No test features found for non-recursive category {category_name}\")\n",
    "        else:\n",
    "            print(f\"   Warning: No non-recursive model found for category {category_name}\")\n",
    "\n",
    "\n",
    "    if nonrecursive_preds_list:\n",
    "         nonrecursive_preds_df = pd.concat(nonrecursive_preds_list).reset_index(drop=True)\n",
    "         nonrecursive_preds_final = nonrecursive_preds_df.pivot(index='category', columns='week_id', values=TARGET)\n",
    "         nonrecursive_preds_final.columns = [f'F{i}' for i in range(1, P_HORIZON + 1)]\n",
    "         print(\" Non-recursive predictions generated.\")\n",
    "    else:\n",
    "         print(\" ERROR: No non-recursive predictions were generated.\")\n",
    "         nonrecursive_preds_final = None # Or handle error as appropriate\n",
    "\n",
    "    del nonrecursive_preds_list, test_mask, model_nr, category_test_features, grid_df['category_encoded']; gc.collect() # Очистка\n",
    "\n",
    "    # 2. Рекурсивный Прогноз\n",
    "    print(\"\\n Generating recursive predictions...\")\n",
    "    recursive_test_path = os.path.join(processed_data_dir, 'test_recursive_arxiv.pkl')\n",
    "    base_test = pd.read_pickle(recursive_test_path)\n",
    "    base_test['category_encoded'] = base_test['category'].astype('category').cat.codes.astype('int16') # Encode category\n",
    "    base_test = reduce_mem_usage(base_test, verbose=False)\n",
    "\n",
    "    all_recursive_preds = []\n",
    "    rolling_features_to_recalc = [col for col in features_to_use if 'rolling' in col]\n",
    "    lags_to_recalc = [col for col in features_to_use if f'{TARGET}_lag_' in col] # Все лаги зависят от предсказаний\n",
    "\n",
    "    predict_start_time = time.time()\n",
    "    for predict_step in range(1, P_HORIZON + 1):\n",
    "        current_pred_week_abs = FIRST_PRED_WEEK + predict_step - 1\n",
    "        print(f\"  Predicting recursive | Step: {predict_step}/{P_HORIZON} (Week {current_pred_week_abs})\")\n",
    "        step_start_time = time.time()\n",
    "\n",
    "        # Создаем маску для текущего дня\n",
    "        day_mask_abs = base_test['absolute_week_id'] == current_pred_week_abs\n",
    "\n",
    "        # Подготовка данных для предсказания (вычисляем признаки ДО самого предсказания)\n",
    "        current_features_df = base_test[day_mask_abs].copy()\n",
    "\n",
    "        # Пересчет признаков для predict_step > 1\n",
    "        if predict_step > 1:\n",
    "            print(f\"   Recalculating features for step {predict_step}...\")\n",
    "            # Пересчет лагов (используем обновленный base_test)\n",
    "            for lag in lag_weeks:\n",
    "                lag_col_name = f'{TARGET}_lag_{lag}'\n",
    "                if lag_col_name in base_test.columns:\n",
    "                     # Shift from the original base_test series, using the updated TARGET column\n",
    "                     shifted_values = base_test.groupby('category')[TARGET].shift(lag)\n",
    "                     # Update only the rows corresponding to the current prediction week\n",
    "                     current_features_df[lag_col_name] = shifted_values.loc[current_features_df.index]\n",
    "\n",
    "            # Пересчет скользящих окон\n",
    "            for window in rolling_windows:\n",
    "                # Only recalc those needed based on previous preds\n",
    "                 mean_col_name = f'rolling_mean_{lag_for_roll}_{window}'\n",
    "                 std_col_name = f'rolling_std_{lag_for_roll}_{window}'\n",
    "\n",
    "                 if mean_col_name in rolling_features_to_recalc:\n",
    "                    # Shift and roll on the *updated* base_test TARGET\n",
    "                    rolled_mean = base_test.groupby('category')[TARGET].shift(lag_for_roll).rolling(window, min_periods=1).mean()\n",
    "                    current_features_df[mean_col_name] = rolled_mean.loc[current_features_df.index]\n",
    "                 if std_col_name in rolling_features_to_recalc:\n",
    "                    rolled_std = base_test.groupby('category')[TARGET].shift(lag_for_roll).rolling(window, min_periods=2).std()\n",
    "                    current_features_df[std_col_name] = rolled_std.loc[current_features_df.index].fillna(0) # fillna if std is NaN\n",
    "\n",
    "            current_features_df = reduce_mem_usage(current_features_df, verbose=False)\n",
    "\n",
    "        current_step_preds_data = [] # Данные для записи в основной base_test\n",
    "        # Предсказание по категориям\n",
    "        for category_name in categories:\n",
    "            if category_name in trained_models_recursive:\n",
    "                 model_r = trained_models_recursive[category_name]\n",
    "                 category_code = current_features_df.loc[current_features_df['category']==category_name, 'category_encoded'].iloc[0] # Получаем код\n",
    "                 mask_pred = (current_features_df['category_encoded'] == category_code)\n",
    "\n",
    "                 if mask_pred.sum() > 0:\n",
    "                      features_for_pred = current_features_df.loc[mask_pred, features_to_use]\n",
    "                      # Проверим на NaN перед предсказанием (можно заменить на 0 или среднее)\n",
    "                      if features_for_pred.isnull().any().any():\n",
    "                            print(f\"    Warning: NaNs found in features for {category_name} at step {predict_step}. Filling with 0.\")\n",
    "                            features_for_pred = features_for_pred.fillna(0)\n",
    "                      predictions = model_r.predict(features_for_pred, num_iteration=model_r.best_iteration)\n",
    "                      current_step_preds_data.append(pd.DataFrame({\n",
    "                          'index': current_features_df.loc[mask_pred].index,\n",
    "                          TARGET: predictions.clip(0),\n",
    "                          'category': category_name, # Для сводки\n",
    "                          'absolute_week_id': current_pred_week_abs, # Для сводки\n",
    "                          'week_id': predict_step # Для сводки\n",
    "                      }))\n",
    "\n",
    "        # Обновление base_test ПЕРЕД следующим шагом рекурсии\n",
    "        if current_step_preds_data:\n",
    "             current_preds_df = pd.concat(current_step_preds_data).set_index('index')\n",
    "             base_test.loc[current_preds_df.index, TARGET] = current_preds_df[TARGET]\n",
    "             # Добавляем предсказания текущего шага в общий список\n",
    "             all_recursive_preds.append(current_preds_df[['category', 'absolute_week_id', 'week_id', TARGET]])\n",
    "        else:\n",
    "            print(f\"   Warning: No predictions made for step {predict_step}\")\n",
    "\n",
    "\n",
    "        print(f\"   Week {predict_step} processed in {(time.time() - step_start_time):.2f} sec\")\n",
    "        gc.collect()\n",
    "\n",
    "    # Сборка рекурсивных прогнозов\n",
    "    if all_recursive_preds:\n",
    "        recursive_preds_long = pd.concat(all_recursive_preds).reset_index(drop=True)\n",
    "        recursive_preds_final = recursive_preds_long.pivot(index='category', columns='week_id', values=TARGET)\n",
    "        recursive_preds_final.columns = [f'F{i}' for i in range(1, P_HORIZON + 1)]\n",
    "        print(\" Recursive predictions generated.\")\n",
    "    else:\n",
    "        print(\" ERROR: No recursive predictions were generated.\")\n",
    "        recursive_preds_final = None # Handle error\n",
    "\n",
    "    del base_test, recursive_preds_long; gc.collect()\n",
    "    print(f\"\\n--- Stage 3: Completed in {(time.time() - overall_start_time)/60:.2f} min ---\")\n",
    "    return recursive_preds_final, nonrecursive_preds_final\n",
    "\n",
    "# --- Этап 4: Ансамблирование ---\n",
    "def run_ensembling(recursive_preds_final, nonrecursive_preds_final):\n",
    "    print(\"\\n--- Starting Stage 4: Ensembling ---\")\n",
    "    if recursive_preds_final is None or nonrecursive_preds_final is None:\n",
    "        print(\" ERROR: One or both prediction sets are missing. Cannot ensemble.\")\n",
    "        return None\n",
    "\n",
    "    # Убедимся, что обе таблицы имеют одинаковые индексы и колонки\n",
    "    common_categories = recursive_preds_final.index.intersection(nonrecursive_preds_final.index)\n",
    "    recursive_preds_final = recursive_preds_final.loc[common_categories]\n",
    "    nonrecursive_preds_final = nonrecursive_preds_final.loc[common_categories]\n",
    "    print(f\"Ensembling predictions for {len(common_categories)} categories.\")\n",
    "\n",
    "    ensembled_preds = (recursive_preds_final + nonrecursive_preds_final) / 2.0\n",
    "    print(\"Ensemble created (simple average).\")\n",
    "    print(f\"--- Stage 4: Completed ---\")\n",
    "    return ensembled_preds\n",
    "\n",
    "# --- Этап 5: Формирование Итогового Файла ---\n",
    "def format_submission(ensembled_preds, test_template_path, output_path):\n",
    "    print(\"\\n--- Starting Stage 5: Formatting Final Submission ---\")\n",
    "    if ensembled_preds is None:\n",
    "        print(\" ERROR: Ensembled predictions are missing. Cannot format submission.\")\n",
    "        return\n",
    "\n",
    "    final_sub_long = ensembled_preds.stack().reset_index()\n",
    "    final_sub_long.columns = ['category', 'forecast_week_col', TARGET]\n",
    "\n",
    "    final_sub_long['week_id'] = final_sub_long['forecast_week_col'].str[1:].astype(int)\n",
    "    final_sub_long['id'] = final_sub_long['category'] + '__' + final_sub_long['week_id'].astype(str)\n",
    "\n",
    "    final_submission_df = final_sub_long[['id', TARGET]]\n",
    "    final_submission_df.rename(columns={TARGET: 'num_papers'}, inplace=True)\n",
    "\n",
    "    # Загрузим оригинальный test-7 для гарантии всех ID и правильного порядка\n",
    "    submission_template = pd.read_csv(test_template_path)\n",
    "    # Убедимся, что в id есть только 'category__week_id'\n",
    "    if 'category' in submission_template.columns and 'week_id' in submission_template.columns:\n",
    "        submission_template['id'] = submission_template['category'] + '__' + submission_template['week_id'].astype(str)\n",
    "        submission_template = submission_template[['id']].copy() # Оставляем только id\n",
    "\n",
    "    final_submission = submission_template.merge(final_submission_df, on='id', how='left')\n",
    "    # Заполним пропуски (если какие-то категории не предсказались) нулями\n",
    "    final_submission['num_papers'].fillna(0, inplace=True)\n",
    "    # Убедимся, что все неотрицательное\n",
    "    final_submission['num_papers'] = final_submission['num_papers'].clip(0)\n",
    "\n",
    "    # Округление? Иногда требуется целое число\n",
    "    # final_submission['num_papers'] = final_submission['num_papers'].round().astype(int)\n",
    "\n",
    "    final_submission.to_csv(output_path, index=False)\n",
    "    print(f\"Final submission saved to {output_path}\")\n",
    "    print(final_submission.head())\n",
    "    print(f\"--- Stage 5: Completed ---\")\n",
    "    return final_submission\n",
    "\n",
    "# --- Основной Пайплайн ---\n",
    "if __name__ == '__main__':\n",
    "    total_pipeline_start_time = time.time()\n",
    "\n",
    "    grid_df, model_features, categorical_for_lgbm, categories = run_preprocessing()\n",
    "    trained_models_r, trained_models_nr, features_to_use, _ = run_training(grid_df, model_features, categorical_for_lgbm, categories)\n",
    "    recursive_preds, nonrecursive_preds = run_prediction(trained_models_r, trained_models_nr, features_to_use, categories)\n",
    "    ensemble_predictions = run_ensembling(recursive_preds, nonrecursive_preds)\n",
    "    final_sub = format_submission(\n",
    "        ensemble_predictions,\n",
    "        os.path.join(raw_data_dir, 'test-7.csv'),\n",
    "        os.path.join(submission_dir, 'submission_ensemble_arxiv_final.csv')\n",
    "    )\n",
    "\n",
    "    print(f\"\\n Pipeline Finished. Total execution time: {(time.time() - total_pipeline_start_time)/60:.2f} minutes.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
