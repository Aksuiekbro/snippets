{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d28dc39",
   "metadata": {},
   "source": [
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "\n",
    "data = pd.read_csv(\"arxiv_data/arxiv_dataset.csv\", index_col=0)\n",
    "\n",
    "X_train = data.loc[:2999, 'sampled_sentence']\n",
    "X_test = data.loc[3000:, 'sampled_sentence']\n",
    "y_train = data.loc[:2999, 'paper_section']\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–µ–º–º–µ—Ä\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# –ö–∞—Å—Ç–æ–º–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è + —Å—Ç–µ–º–º–∏–Ω–≥\n",
    "def tokenize_and_stem(text):\n",
    "    # –ë–∞–∑–æ–≤–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = text.strip().split()\n",
    "    return [stemmer.stem(token) for token in tokens if len(token) > 2]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=tokenize_and_stem,\n",
    "    ngram_range=(1, 2),          # unigrams + bigrams\n",
    "    stop_words='english',        # —É–±–∏—Ä–∞–µ–º —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞\n",
    "    max_df=0.06,                 # —É–±–∏—Ä–∞–µ–º —Å—É–ø–µ—Ä–≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞\n",
    ")\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)  # sparse matrix\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d510a2dd",
   "metadata": {},
   "source": [
    "üìå 1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (Feature Extraction)\n",
    "üìç CountVectorizer (–¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152b55fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texts = [\"I love cats\", \"Cats are lovely\"]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())  # ['are' 'cats' 'love' 'lovely']\n",
    "print(X.toarray())  # [[0 1 1 0], [1 1 0 1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d875c118",
   "metadata": {},
   "source": [
    "üìç TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5248e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "texts = [\"I love cats\", \"Cats are lovely\"]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878dd49a",
   "metadata": {},
   "source": [
    "üìç PCA (—É–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229db215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X = load_iris().data\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "print(X_reduced[:5])  # –°–æ–∫—Ä–∞—â—ë–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (2 –ø—Ä–∏–∑–Ω–∞–∫–∞)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35fd9d9",
   "metadata": {},
   "source": [
    "üìç groupby –≤ pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8500ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'user': ['A', 'A', 'B', 'B'],\n",
    "    'score': [10, 20, 30, 40]\n",
    "})\n",
    "\n",
    "agg = df.groupby('user')['score'].mean().reset_index()\n",
    "print(agg)  # –°—Ä–µ–¥–Ω–∏–π score –ø–æ user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b62875",
   "metadata": {},
   "source": [
    "–í—ã–≤–æ–¥:\n",
    "  store  day  sales\n",
    "0     A  Mon    100\n",
    "1     A  Tue    150\n",
    "2     B  Mon    200\n",
    "3     B  Tue    180\n",
    "4     B  Wed    160\n",
    "5     C  Mon     90\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6c1d2c",
   "metadata": {},
   "source": [
    "üìå –¶–µ–ª—å: –Ω–∞–π—Ç–∏ —Å—Ä–µ–¥–Ω—é—é –≤—ã—Ä—É—á–∫—É –ø–æ –∫–∞–∂–¥–æ–º—É –º–∞–≥–∞–∑–∏–Ω—É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b86ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –º–∞–≥–∞–∑–∏–Ω—É –∏ —Å—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω—é—é –≤—ã—Ä—É—á–∫—É\n",
    "avg_sales = data.groupby('store')['sales'].mean().reset_index()\n",
    "avg_sales.columns = ['store', 'avg_sales']\n",
    "\n",
    "print(avg_sales)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e568a",
   "metadata": {},
   "source": [
    "  store  avg_sales\n",
    "0     A      125.0\n",
    "1     B      180.0\n",
    "2     C       90.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efdffea",
   "metadata": {},
   "source": [
    "üîÅ –ï—â—ë –ø—Ä–∏–º–µ—Ä: –æ–±—â–∞—è –≤—ã—Ä—É—á–∫–∞ –ø–æ –∫–∞–∂–¥–æ–º—É –º–∞–≥–∞–∑–∏–Ω—É –∏ –¥–Ω—é"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa49f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = data.groupby(['store', 'day'])['sales'].sum().reset_index()\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47bd0bb",
   "metadata": {},
   "source": [
    "  store  day  sales\n",
    "0     A  Mon    100\n",
    "1     A  Tue    150\n",
    "2     B  Mon    200\n",
    "3     B  Tue    180\n",
    "4     B  Wed    160\n",
    "5     C  Mon     90\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a108629",
   "metadata": {},
   "source": [
    "–û—Ç–ª–∏—á–Ω–æ! –ù–∏–∂–µ –ø–æ–∫–∞–∂—É 2 –º–æ—â–Ω—ã—Ö –ø—Ä–∏—ë–º–∞ —Å `groupby` ‚Äî –¥–ª—è **feature engineering** –≤ ML-–∑–∞–¥–∞—á–∞—Ö:\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 1. `groupby + transform`: —Å–æ–∑–¥–∞—ë–º —Ñ–∏—á—É \"–æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ –≤ –≥—Ä—É–ø–ø–µ\"\n",
    "\n",
    "–î–æ–ø—É—Å—Ç–∏–º, —É –Ω–∞—Å –µ—Å—Ç—å –∑–∞–∫–∞–∑—ã –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤ —Ä–∞–∑–Ω—ã—Ö –≥–æ—Ä–æ–¥–∞—Ö:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'city': ['A', 'A', 'B', 'B', 'C', 'C'],\n",
    "    'user_id': [1, 2, 3, 4, 5, 6],\n",
    "    'order_amount': [100, 150, 200, 250, 80, 120]\n",
    "})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìå –¶–µ–ª—å: –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ –Ω–∞–π—Ç–∏ **–æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ –ø–æ –≥–æ—Ä–æ–¥—É**\n",
    "\n",
    "```python\n",
    "# –°—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π —á–µ–∫ –ø–æ –≥–æ—Ä–æ–¥—É\n",
    "df['city_avg'] = df.groupby('city')['order_amount'].transform('mean')\n",
    "\n",
    "# –í—ã—á–∏—Å–ª—è–µ–º –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ\n",
    "df['dev_from_city_avg'] = df['order_amount'] - df['city_avg']\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "**–í—ã–≤–æ–¥:**\n",
    "\n",
    "```\n",
    "  city  user_id  order_amount  city_avg  dev_from_city_avg\n",
    "0    A        1           100     125.0              -25.0\n",
    "1    A        2           150     125.0               25.0\n",
    "2    B        3           200     225.0              -25.0\n",
    "3    B        4           250     225.0               25.0\n",
    "4    C        5            80     100.0              -20.0\n",
    "5    C        6           120     100.0               20.0\n",
    "```\n",
    "\n",
    "üëâ –≠—Ç–æ –ø–æ–ª–µ–∑–Ω–æ, –µ—Å–ª–∏ —Ç—ã —Ö–æ—á–µ—à—å —É—á–∏—Ç—ã–≤–∞—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤—ã—à–µ –∏–ª–∏ –Ω–∏–∂–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –ø–æ –≥—Ä—É–ø–ø–µ).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 2. `groupby + apply`: —Å–æ–∑–¥–∞—ë–º —Å–ª–æ–∂–Ω—É—é —Ñ–∏—á—É ‚Äî —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–Ω—É—Ç—Ä–∏ –≥—Ä—É–ø–ø—ã\n",
    "\n",
    "```python\n",
    "# –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º —Ä–∞–Ω–≥–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –ø–æ —Å—É–º–º–µ –∑–∞–∫–∞–∑–æ–≤ –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ –≥–æ—Ä–æ–¥–∞\n",
    "df['rank_in_city'] = df.groupby('city')['order_amount'].apply(lambda x: x.rank(ascending=False))\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "**–í—ã–≤–æ–¥:**\n",
    "\n",
    "```\n",
    "  city  user_id  order_amount  rank_in_city\n",
    "0    A        1           100           2.0\n",
    "1    A        2           150           1.0\n",
    "2    B        3           200           2.0\n",
    "3    B        4           250           1.0\n",
    "4    C        5            80           2.0\n",
    "5    C        6           120           1.0\n",
    "```\n",
    "\n",
    "üìç –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö –∏–ª–∏ –∫—Ä–µ–¥–∏—Ç–Ω–æ–º —Å–∫–æ—Ä–∏–Ω–≥–µ: –∫—Ç–æ –ª—É—á—à–∏–π –≤ —Å–≤–æ–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏?\n",
    "\n",
    "---\n",
    "\n",
    "–•–æ—á–µ—à—å, –ø–æ–∫–∞–∂—É, –∫–∞–∫ —ç—Ç–∏ —Ñ–∏—á–∏ –º–æ–∂–Ω–æ –ø–æ—Ç–æ–º –ø–µ—Ä–µ–¥–∞—Ç—å –≤ –º–æ–¥–µ–ª—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, LightGBM)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad28c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞\n",
    "data = pd.DataFrame({\n",
    "    'store': ['A', 'A', 'B', 'B', 'B', 'C'],\n",
    "    'day': ['Mon', 'Tue', 'Mon', 'Tue', 'Wed', 'Mon'],\n",
    "    'sales': [100, 150, 200, 180, 160, 90]\n",
    "})\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d59fba1",
   "metadata": {},
   "source": [
    "üìç –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥ (–Ω–∞ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e380273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=100, n_features=4)\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.predict(X[:5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb3853f",
   "metadata": {},
   "source": [
    "üìç KFold –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "X = np.arange(10)\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    print(\"Train:\", train_index, \"Val:\", val_index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
