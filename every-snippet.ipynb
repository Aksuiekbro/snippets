{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"NLP","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np, torch\n\ntexts, labels = [...], [...]  # your dataset\n\ntok = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2').eval()\n\ndef get_emb(text):\n    with torch.no_grad():\n        t = tok(text, return_tensors='pt', truncation=True, padding=True)\n        return model(**t).last_hidden_state[:,0].squeeze().numpy()\n\nX = np.vstack([get_emb(t) for t in texts])\nclf = RandomForestClassifier().fit(X, labels)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ðŸ§  NLP Pipeline #1: TF-IDF + Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\ntfidf_lr = Pipeline([\n    ('tfidf', TfidfVectorizer(\n        lowercase=True, stop_words='english',\n        ngram_range=(1, 2), max_df=0.8, min_df=5)),\n    ('clf', LogisticRegression(max_iter=1000))\n])\n\ntfidf_lr.fit(texts, labels)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2 TRANSFORMERS","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¿Ñ€Ð¸Ð¼ÐµÑ€ CSV-Ñ„Ð°Ð¹Ð»Ð°\n# ÐŸÑ€ÐµÐ´Ð¿Ð¾Ð»Ð¾Ð¶Ð¸Ð¼, Ñ‡Ñ‚Ð¾ Ð² CSV Ð´Ð²Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸: \"text\" Ð¸ \"label\"\nsample_data = {\n    \"text\": [\n        \"This product is amazing!\",\n        \"Terrible experience, will not buy again.\",\n        \"Not bad, but not great either.\",\n        \"Absolutely fantastic! Exceeded expectations.\",\n        \"Waste of money, very disappointed.\"\n    ],\n    \"label\": [1, 0, 1, 1, 0]\n}\n\ndf = pd.DataFrame(sample_data)\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ BERT\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nbert = AutoModel.from_pretrained('bert-base-uncased').eval()\n\n# ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ CLS embedding\ndef get_cls_emb(text):\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n    with torch.no_grad():\n        out = bert(**inputs)\n    return out.last_hidden_state[:, 0].squeeze().numpy()  # [CLS] Ñ‚Ð¾ÐºÐµÐ½\n\n# ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸\nX_emb = np.vstack([get_cls_emb(t) for t in df[\"text\"]])\ny = df[\"label\"].values\n\n# Ð”ÐµÐ»Ð¸Ð¼ Ð½Ð° train/test\nX_train, X_test, y_train, y_test = train_test_split(X_emb, y, test_size=0.2, random_state=42)\n\n# ÐžÐ±ÑƒÑ‡Ð°ÐµÐ¼ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# ÐžÑ†ÐµÐ½Ð¸Ð²Ð°ÐµÐ¼\ny_pred = clf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\naccuracy\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ðŸ–¼ï¸ CV â€” Image Classification (ResNet + torchvision) Ð•Ð¡Ð›Ð˜ Ð¿Ð¾ Ð¿Ð°Ð¿ÐºÐ°Ð¼","metadata":{}},{"cell_type":"code","source":"import torch, torchvision\nfrom torchvision import datasets, transforms, models\nfrom torch import nn, optim\n\ntrain_tf = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor()\n])\n\ntrain_data = datasets.ImageFolder('train_dir/', transform=train_tf)\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n\nmodel = models.resnet18(pretrained=True)\nmodel.fc = nn.Linear(model.fc.in_features, len(train_data.classes))  # new head\nmodel = model.cuda()\n\ncriterion = nn.CrossEntropyLoss()\nopt = optim.Adam(model.parameters(), lr=1e-3)\n\nfor epoch in range(5):\n    for X, y in train_loader:\n        X, y = X.cuda(), y.cuda()\n        opt.zero_grad()\n        loss = criterion(model(X), y)\n        loss.backward()\n        opt.step()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"CV - Image classification with csv format","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# 1ï¸âƒ£ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸ Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¸Ð½Ð³ CSV\ndf = pd.read_csv('train.csv')                      # Ð¤Ð°Ð¹Ð» Ñ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ°Ð¼Ð¸: label, pixel0...pixel783\nX = df.drop('label', axis=1).values / 255.0        # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ [0,1]\ny = df['label'].values\n\n# 2ï¸âƒ£ ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð² Ñ„Ð¾Ñ€Ð¼Ñƒ [N, 1, 28, 28]\nX = X.reshape(-1, 1, 28, 28).astype(np.float32)\ny = y.astype(np.int64)\n\n# 3ï¸âƒ£ Ð Ð°Ð·Ð´ÐµÐ»Ð¸Ð¼ Ð½Ð° train / val\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 4ï¸âƒ£ DataLoader'Ñ‹\ntrain_ds = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\nval_ds   = TensorDataset(torch.tensor(X_val),   torch.tensor(y_val))\ntrain_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\nval_dl   = DataLoader(val_ds, batch_size=64)\n\n# 5ï¸âƒ£ ÐŸÑ€Ð¾ÑÑ‚Ð°Ñ CNN\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2),  # 28x28 â†’ 14x14\n            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2),  # 14x14 â†’ 7x7\n            nn.Flatten(),\n            nn.Linear(32 * 7 * 7, 128), nn.ReLU(),\n            nn.Linear(128, 10)  # 10 ÐºÐ»Ð°ÑÑÐ¾Ð²\n        )\n    def forward(self, x): return self.net(x)\n\nmodel = CNN().cuda()\nopt = torch.optim.Adam(model.parameters(), lr=1e-3)\nloss_fn = nn.CrossEntropyLoss()\n\n# 6ï¸âƒ£ ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ\nfor epoch in range(5):\n    model.train()\n    for Xb, yb in train_dl:\n        Xb, yb = Xb.cuda(), yb.cuda()\n        opt.zero_grad()\n        loss = loss_fn(model(Xb), yb)\n        loss.backward()\n        opt.step()\n\n    # Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ\n    model.eval(); correct = 0; total = 0\n    with torch.no_grad():\n        for Xb, yb in val_dl:\n            Xb, yb = Xb.cuda(), yb.cuda()\n            pred = model(Xb).argmax(1)\n            correct += (pred == yb).sum().item()\n            total += yb.size(0)\n    acc = correct / total\n    print(f'Epoch {epoch+1}: val acc = {acc:.4f}')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ðŸ“Š Tabular â€” XGBoost Ñ Optuna (Ð±Ð¸Ð½Ð°Ñ€Ð½Ð°Ñ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import xgboost as xgb, optuna\nfrom sklearn.model_selection import train_test_split\n\nX, y = ... , ...  # your features and targets\nX_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\ndef objective(trial):\n    params = {\n        'objective': 'binary:logistic', 'eval_metric': 'auc', 'tree_method': 'hist',\n        'eta': trial.suggest_float('eta', 0.01, 0.3),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'subsample': trial.suggest_float('subsample', 0.5, 1),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1)\n    }\n    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n    cv = xgb.cv(params, dtrain, nfold=3, num_boost_round=200,\n                early_stopping_rounds=20, seed=42)\n    return cv['test-auc-mean'].max()\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=20)\n\nfinal_model = xgb.XGBClassifier(**study.best_params)\nfinal_model.fit(X_tr, y_tr)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Basic -> EDA","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}