{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc841c5",
   "metadata": {},
   "source": [
    "Конечно! Если у вас есть **10 признаков (фич)** в датафрейме, и вы применяете **PCA** для уменьшения размерности, то **финальный датасет** будет иметь количество признаков, равное **числу выбранных главных компонент**.\n",
    "\n",
    "### Пример:\n",
    "\n",
    "Предположим, у вас есть датафрейм с **10 признаками**, и вы хотите уменьшить размерность до **2 компонент** с помощью PCA. После применения PCA:\n",
    "\n",
    "* **Исходный датасет** с 10 признаками (фичами) будет иметь форму **(n\\_samples, 10)**, где `n_samples` — количество строк (образцов) в данных.\n",
    "* После применения PCA для уменьшения размерности до 2 компонент, **финальный датасет** будет иметь форму **(n\\_samples, 2)**, где 2 — это количество главных компонент, которые вы выбрали.\n",
    "\n",
    "### Как это будет выглядеть:\n",
    "\n",
    "1. **Исходный датасет (до PCA)**:\n",
    "\n",
    "   * Пусть у вас есть датафрейм с 1000 строк и 10 признаками:\n",
    "\n",
    "   ```python\n",
    "   df = pd.DataFrame(np.random.rand(1000, 10), columns=[f\"Feature_{i+1}\" for i in range(10)])\n",
    "   print(df.shape)  # (1000, 10)\n",
    "   ```\n",
    "\n",
    "2. **Применение PCA для уменьшения размерности**:\n",
    "\n",
    "   * Вы хотите уменьшить количество признаков до 2, применяя PCA:\n",
    "\n",
    "   ```python\n",
    "   from sklearn.decomposition import PCA\n",
    "   from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "   # Стандартизируем данные\n",
    "   scaler = StandardScaler()\n",
    "   df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "   # Применяем PCA\n",
    "   pca = PCA(n_components=2)\n",
    "   df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "   print(df_pca.shape)  # (1000, 2)\n",
    "   ```\n",
    "\n",
    "3. **Финальный датасет (после PCA)**:\n",
    "\n",
    "   * Новый датафрейм после применения PCA будет иметь форму **(1000, 2)**, где 2 — это количество главных компонент:\n",
    "\n",
    "   ```python\n",
    "   df_pca_df = pd.DataFrame(df_pca, columns=[\"PC1\", \"PC2\"])\n",
    "   print(df_pca_df.head())\n",
    "   ```\n",
    "\n",
    "### Примерный вид финального датафрейма:\n",
    "\n",
    "```python\n",
    "    PC1       PC2\n",
    "0   1.1323    -0.7541\n",
    "1   -0.4625   0.3472\n",
    "2   0.8823    1.0542\n",
    "3   -1.2042   -0.9023\n",
    "4   0.9923    0.0024\n",
    "```\n",
    "\n",
    "### Примечания:\n",
    "\n",
    "1. **PC1** и **PC2** — это первые две главные компоненты, которые объясняют наибольшую дисперсию данных. В зависимости от вашего выбора, вы можете использовать больше или меньше компонент.\n",
    "\n",
    "2. **Объясненная дисперсия** — это важный момент при применении PCA. Например, первые 2 компоненты могут объяснять 90% дисперсии в данных, а оставшиеся компоненты могут объяснять лишь 10%. Это поможет вам понять, сколько информации вы теряете при уменьшении размерности.\n",
    "\n",
    "### Вывод:\n",
    "\n",
    "* В конечном итоге ваш датасет с 10 признаками будет преобразован в датасет с 2 признаками (главными компонентами), которые максимально сохраняют информацию о вариативности данных.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
